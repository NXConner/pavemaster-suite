# PHASE 14: Advanced CI/CD Pipeline
# Comprehensive CI/CD with testing, security, performance, and deployment automation
name: 'PaveMaster Suite - Advanced CI/CD Pipeline'

on:
  push:
    branches: [ main, develop, 'feature/*', 'hotfix/*', 'release/*' ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      deployment_strategy:
        description: 'Deployment strategy'
        required: true
        default: 'rolling'
        type: choice
        options:
          - rolling
          - blue_green
          - canary
      skip_tests:
        description: 'Skip test execution'
        required: false
        default: false
        type: boolean

env:
  NODE_VERSION: '20'
  PNPM_VERSION: '8'
  DOCKER_REGISTRY: ghcr.io
  IMAGE_NAME: pavemaster-suite
  
  # Security scanning
  SECURITY_SCAN_ENABLED: true
  DEPENDENCY_CHECK_ENABLED: true
  SAST_ENABLED: true
  CONTAINER_SCAN_ENABLED: true
  
  # Performance testing
  PERFORMANCE_TEST_ENABLED: true
  LOAD_TEST_ENABLED: true
  
  # Quality gates
  COVERAGE_THRESHOLD: 80
  QUALITY_GATE_ENABLED: true
  
  # Deployment
  DEPLOYMENT_TIMEOUT: 600
  HEALTH_CHECK_TIMEOUT: 300
  ROLLBACK_ENABLED: true

jobs:
  # PHASE 14: Pre-flight Checks and Setup
  setup:
    name: 'Setup & Environment Validation'
    runs-on: ubuntu-latest
    outputs:
      should_deploy: ${{ steps.deployment_check.outputs.should_deploy }}
      target_environment: ${{ steps.deployment_check.outputs.environment }}
      deployment_strategy: ${{ steps.deployment_check.outputs.strategy }}
      version: ${{ steps.version.outputs.version }}
      short_sha: ${{ steps.version.outputs.short_sha }}
    
    steps:
      - name: 'Checkout Code'
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: 'Validate Branch Strategy'
        id: branch_validation
        run: |
          BRANCH_NAME=${GITHUB_HEAD_REF:-${GITHUB_REF#refs/heads/}}
          echo "branch=$BRANCH_NAME" >> $GITHUB_OUTPUT
          
          # Validate branch naming conventions
          if [[ "$BRANCH_NAME" =~ ^(main|develop|feature\/[a-z0-9-]+|hotfix\/[a-z0-9-]+|release\/[0-9]+\.[0-9]+\.[0-9]+)$ ]]; then
            echo "‚úÖ Branch name follows convention: $BRANCH_NAME"
            echo "valid=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Invalid branch name: $BRANCH_NAME"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: 'Generate Version'
        id: version
        run: |
          SHORT_SHA=$(git rev-parse --short HEAD)
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            VERSION=$(git describe --tags --abbrev=0 2>/dev/null || echo "v1.0.0")
            VERSION="${VERSION}-${SHORT_SHA}"
          elif [[ "${{ github.ref }}" == "refs/heads/develop" ]]; then
            VERSION="develop-${TIMESTAMP}-${SHORT_SHA}"
          else
            BRANCH_NAME=$(echo "${{ steps.branch_validation.outputs.branch }}" | sed 's/[^a-zA-Z0-9-]/-/g')
            VERSION="${BRANCH_NAME}-${TIMESTAMP}-${SHORT_SHA}"
          fi
          
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "short_sha=$SHORT_SHA" >> $GITHUB_OUTPUT
          echo "üè∑Ô∏è Generated version: $VERSION"

      - name: 'Deployment Strategy Check'
        id: deployment_check
        run: |
          SHOULD_DEPLOY=false
          ENVIRONMENT="none"
          STRATEGY="rolling"
          
          # Determine deployment based on branch and triggers
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            SHOULD_DEPLOY=true
            ENVIRONMENT="${{ github.event.inputs.environment }}"
            STRATEGY="${{ github.event.inputs.deployment_strategy }}"
          elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            SHOULD_DEPLOY=true
            ENVIRONMENT="production"
            STRATEGY="blue_green"
          elif [[ "${{ github.ref }}" == "refs/heads/develop" ]]; then
            SHOULD_DEPLOY=true
            ENVIRONMENT="staging"
            STRATEGY="rolling"
          fi
          
          echo "should_deploy=$SHOULD_DEPLOY" >> $GITHUB_OUTPUT
          echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
          echo "strategy=$STRATEGY" >> $GITHUB_OUTPUT
          echo "üéØ Deployment: $SHOULD_DEPLOY to $ENVIRONMENT using $STRATEGY"

      - name: 'Environment Validation'
        run: |
          echo "üîç Validating environment configuration..."
          echo "Node.js Version: ${{ env.NODE_VERSION }}"
          echo "PNPM Version: ${{ env.PNPM_VERSION }}"
          echo "Docker Registry: ${{ env.DOCKER_REGISTRY }}"
          echo "Security Scanning: ${{ env.SECURITY_SCAN_ENABLED }}"
          echo "Performance Testing: ${{ env.PERFORMANCE_TEST_ENABLED }}"
          echo "Coverage Threshold: ${{ env.COVERAGE_THRESHOLD }}%"

  # PHASE 14: Code Quality and Security Analysis
  code_analysis:
    name: 'Code Quality & Security Analysis'
    runs-on: ubuntu-latest
    needs: setup
    if: always() && needs.setup.result == 'success'
    
    steps:
      - name: 'Checkout Code'
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: 'Setup Node.js'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 'Setup PNPM'
        uses: pnpm/action-setup@v2
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: 'Install Dependencies'
        run: |
          pnpm install --frozen-lockfile
          echo "üì¶ Dependencies installed successfully"

      - name: 'Lint Check'
        run: |
          echo "üîç Running ESLint..."
          pnpm run lint
          echo "‚úÖ Linting completed successfully"

      - name: 'Type Check'
        run: |
          echo "üîç Running TypeScript compiler..."
          pnpm run type-check
          echo "‚úÖ Type checking completed successfully"

      - name: 'Code Formatting Check'
        run: |
          echo "üîç Checking code formatting..."
          pnpm run format:check
          echo "‚úÖ Code formatting validated"

      - name: 'Dependency Vulnerability Scan'
        if: env.DEPENDENCY_CHECK_ENABLED == 'true'
        run: |
          echo "üîç Scanning dependencies for vulnerabilities..."
          pnpm audit --audit-level moderate
          npx better-npm-audit audit --level moderate
          echo "‚úÖ Dependency scan completed"

      - name: 'SAST Security Scan'
        if: env.SAST_ENABLED == 'true'
        uses: github/super-linter@v4
        env:
          DEFAULT_BRANCH: main
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          VALIDATE_ALL_CODEBASE: false
          VALIDATE_TYPESCRIPT_ES: true
          VALIDATE_TYPESCRIPT_STANDARD: true
          VALIDATE_JAVASCRIPT_ES: true
          VALIDATE_CSS: true
          VALIDATE_DOCKERFILE: true
          VALIDATE_YAML: true
          VALIDATE_JSON: true

      - name: 'CodeQL Analysis'
        if: env.SAST_ENABLED == 'true'
        uses: github/codeql-action/init@v3
        with:
          languages: typescript, javascript

      - name: 'Perform CodeQL Analysis'
        if: env.SAST_ENABLED == 'true'
        uses: github/codeql-action/analyze@v3

      - name: 'Upload Security Scan Results'
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-scan-results
          path: |
            security-reports/
            .eslintcache
            super-linter.log
          retention-days: 30

  # PHASE 14: Comprehensive Testing Suite
  test_suite:
    name: 'Comprehensive Testing Suite'
    runs-on: ubuntu-latest
    needs: setup
    if: always() && needs.setup.result == 'success' && github.event.inputs.skip_tests != 'true'
    
    strategy:
      matrix:
        test-type: ['unit', 'integration', 'e2e']
        node-version: ['18', '20']
    
    steps:
      - name: 'Checkout Code'
        uses: actions/checkout@v4

      - name: 'Setup Node.js ${{ matrix.node-version }}'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: 'Setup PNPM'
        uses: pnpm/action-setup@v2
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: 'Install Dependencies'
        run: pnpm install --frozen-lockfile

      - name: 'Run Unit Tests'
        if: matrix.test-type == 'unit'
        run: |
          echo "üß™ Running unit tests with coverage..."
          pnpm run test:unit --coverage --watchAll=false
          echo "‚úÖ Unit tests completed"

      - name: 'Run Integration Tests'
        if: matrix.test-type == 'integration'
        run: |
          echo "üß™ Running integration tests..."
          pnpm run test:integration --watchAll=false
          echo "‚úÖ Integration tests completed"

      - name: 'Setup Test Database'
        if: matrix.test-type == 'e2e'
        run: |
          echo "üóÑÔ∏è Setting up test database..."
          docker run -d --name test-db -p 5432:5432 -e POSTGRES_DB=test -e POSTGRES_PASSWORD=test postgres:15
          sleep 10
          echo "‚úÖ Test database ready"

      - name: 'Run E2E Tests'
        if: matrix.test-type == 'e2e'
        run: |
          echo "üß™ Running end-to-end tests..."
          pnpm run test:e2e
          echo "‚úÖ E2E tests completed"

      - name: 'Accessibility Tests'
        if: matrix.test-type == 'e2e'
        run: |
          echo "‚ôø Running accessibility tests..."
          pnpm run test:a11y
          echo "‚úÖ Accessibility tests completed"

      - name: 'Visual Regression Tests'
        if: matrix.test-type == 'e2e'
        run: |
          echo "üëÄ Running visual regression tests..."
          pnpm run test:visual
          echo "‚úÖ Visual regression tests completed"

      - name: 'Coverage Analysis'
        if: matrix.test-type == 'unit'
        run: |
          echo "üìä Analyzing test coverage..."
          COVERAGE=$(cat coverage/coverage-summary.json | jq '.total.lines.pct')
          echo "Coverage: $COVERAGE%"
          
          if (( $(echo "$COVERAGE < ${{ env.COVERAGE_THRESHOLD }}" | bc -l) )); then
            echo "‚ùå Coverage $COVERAGE% is below threshold ${{ env.COVERAGE_THRESHOLD }}%"
            exit 1
          else
            echo "‚úÖ Coverage $COVERAGE% meets threshold ${{ env.COVERAGE_THRESHOLD }}%"
          fi

      - name: 'Upload Test Results'
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.test-type }}-node${{ matrix.node-version }}
          path: |
            coverage/
            test-results/
            screenshots/
            videos/
          retention-days: 30

      - name: 'Upload Coverage to Codecov'
        if: matrix.test-type == 'unit' && matrix.node-version == '20'
        uses: codecov/codecov-action@v3
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./coverage/lcov.info
          flags: unittests
          name: codecov-umbrella

  # PHASE 14: Performance Testing
  performance_tests:
    name: 'Performance Testing'
    runs-on: ubuntu-latest
    needs: [setup, test_suite]
    if: always() && needs.setup.result == 'success' && env.PERFORMANCE_TEST_ENABLED == 'true'
    
    steps:
      - name: 'Checkout Code'
        uses: actions/checkout@v4

      - name: 'Setup Node.js'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 'Install Dependencies'
        run: |
          pnpm install --frozen-lockfile
          npm install -g lighthouse lighthouse-ci

      - name: 'Build Application'
        run: |
          echo "üèóÔ∏è Building application for performance testing..."
          pnpm run build
          echo "‚úÖ Build completed"

      - name: 'Start Application'
        run: |
          echo "üöÄ Starting application server..."
          pnpm run preview &
          sleep 30
          echo "‚úÖ Application server started"

      - name: 'Lighthouse Performance Audit'
        run: |
          echo "üîç Running Lighthouse performance audit..."
          lhci autorun --config=.lighthouserc.json
          echo "‚úÖ Lighthouse audit completed"

      - name: 'Bundle Size Analysis'
        run: |
          echo "üì¶ Analyzing bundle sizes..."
          pnpm run analyze
          
          # Check bundle size limits
          MAIN_BUNDLE_SIZE=$(du -k dist/assets/index-*.js | cut -f1)
          VENDOR_BUNDLE_SIZE=$(du -k dist/assets/vendor-*.js | cut -f1)
          
          echo "Main bundle: ${MAIN_BUNDLE_SIZE}KB"
          echo "Vendor bundle: ${VENDOR_BUNDLE_SIZE}KB"
          
          # Set limits (in KB)
          MAIN_LIMIT=500
          VENDOR_LIMIT=1000
          
          if [ $MAIN_BUNDLE_SIZE -gt $MAIN_LIMIT ]; then
            echo "‚ùå Main bundle size ${MAIN_BUNDLE_SIZE}KB exceeds limit ${MAIN_LIMIT}KB"
            exit 1
          fi
          
          if [ $VENDOR_BUNDLE_SIZE -gt $VENDOR_LIMIT ]; then
            echo "‚ùå Vendor bundle size ${VENDOR_BUNDLE_SIZE}KB exceeds limit ${VENDOR_LIMIT}KB"
            exit 1
          fi
          
          echo "‚úÖ Bundle sizes within limits"

      - name: 'Load Testing'
        if: env.LOAD_TEST_ENABLED == 'true'
        run: |
          echo "‚ö° Running load tests..."
          # Install k6 for load testing
          curl https://github.com/grafana/k6/releases/download/v0.46.0/k6-v0.46.0-linux-amd64.tar.gz -L | tar xvz --strip-components 1
          ./k6 run --vus 10 --duration 30s tests/load/basic-load-test.js
          echo "‚úÖ Load testing completed"

      - name: 'Upload Performance Results'
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: |
            .lighthouseci/
            bundle-analyzer-report.html
            load-test-results.json
          retention-days: 30

  # PHASE 14: Container Build and Security Scan
  container_build:
    name: 'Container Build & Security Scan'
    runs-on: ubuntu-latest
    needs: [setup, code_analysis]
    if: always() && needs.setup.result == 'success'
    
    outputs:
      image_digest: ${{ steps.build.outputs.digest }}
      image_tag: ${{ steps.meta.outputs.tags }}
    
    steps:
      - name: 'Checkout Code'
        uses: actions/checkout@v4

      - name: 'Set up Docker Buildx'
        uses: docker/setup-buildx-action@v3

      - name: 'Log in to Container Registry'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.DOCKER_REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: 'Extract Metadata'
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.DOCKER_REGISTRY }}/${{ github.repository }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=${{ needs.setup.outputs.version }}

      - name: 'Build and Push Container Image'
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            VERSION=${{ needs.setup.outputs.version }}
            BUILD_DATE=${{ github.event.head_commit.timestamp }}
            VCS_REF=${{ github.sha }}

      - name: 'Container Security Scan'
        if: env.CONTAINER_SCAN_ENABLED == 'true'
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.DOCKER_REGISTRY }}/${{ github.repository }}/${{ env.IMAGE_NAME }}:${{ needs.setup.outputs.version }}
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH,MEDIUM'

      - name: 'Upload Container Scan Results'
        if: always() && env.CONTAINER_SCAN_ENABLED == 'true'
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: 'trivy-results.sarif'

      - name: 'Generate Container SBOM'
        run: |
          echo "üìã Generating Software Bill of Materials..."
          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
            anchore/syft:latest \
            ${{ env.DOCKER_REGISTRY }}/${{ github.repository }}/${{ env.IMAGE_NAME }}:${{ needs.setup.outputs.version }} \
            -o spdx-json=sbom.json
          echo "‚úÖ SBOM generated"

      - name: 'Upload Container Artifacts'
        uses: actions/upload-artifact@v4
        with:
          name: container-artifacts
          path: |
            trivy-results.sarif
            sbom.json
          retention-days: 30

  # PHASE 14: Quality Gate Evaluation
  quality_gate:
    name: 'Quality Gate Evaluation'
    runs-on: ubuntu-latest
    needs: [setup, code_analysis, test_suite, performance_tests, container_build]
    if: always() && needs.setup.result == 'success' && env.QUALITY_GATE_ENABLED == 'true'
    
    steps:
      - name: 'Download Test Results'
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-*
          merge-multiple: true

      - name: 'Download Security Results'
        uses: actions/download-artifact@v4
        with:
          name: security-scan-results

      - name: 'Download Performance Results'
        uses: actions/download-artifact@v4
        with:
          name: performance-results

      - name: 'Evaluate Quality Gate'
        run: |
          echo "üö™ Evaluating Quality Gate..."
          
          # Initialize pass/fail counters
          PASSED=0
          TOTAL=0
          
          # Check test coverage
          if [ -f "coverage/coverage-summary.json" ]; then
            COVERAGE=$(cat coverage/coverage-summary.json | jq '.total.lines.pct')
            TOTAL=$((TOTAL + 1))
            if (( $(echo "$COVERAGE >= ${{ env.COVERAGE_THRESHOLD }}" | bc -l) )); then
              echo "‚úÖ Coverage: $COVERAGE% (‚â• ${{ env.COVERAGE_THRESHOLD }}%)"
              PASSED=$((PASSED + 1))
            else
              echo "‚ùå Coverage: $COVERAGE% (< ${{ env.COVERAGE_THRESHOLD }}%)"
            fi
          fi
          
          # Check security vulnerabilities
          TOTAL=$((TOTAL + 1))
          if [ ! -f "trivy-results.sarif" ] || [ $(cat trivy-results.sarif | jq '.runs[0].results | length') -eq 0 ]; then
            echo "‚úÖ No critical security vulnerabilities found"
            PASSED=$((PASSED + 1))
          else
            echo "‚ùå Security vulnerabilities detected"
          fi
          
          # Check performance metrics
          if [ -f ".lighthouseci/manifest.json" ]; then
            PERFORMANCE_SCORE=$(cat .lighthouseci/manifest.json | jq '.[0].summary.performance')
            TOTAL=$((TOTAL + 1))
            if (( $(echo "$PERFORMANCE_SCORE >= 0.8" | bc -l) )); then
              echo "‚úÖ Performance score: $PERFORMANCE_SCORE (‚â• 0.8)"
              PASSED=$((PASSED + 1))
            else
              echo "‚ùå Performance score: $PERFORMANCE_SCORE (< 0.8)"
            fi
          fi
          
          # Evaluate overall quality gate
          echo "üìä Quality Gate Results: $PASSED/$TOTAL checks passed"
          
          if [ $PASSED -eq $TOTAL ]; then
            echo "‚úÖ Quality Gate PASSED"
            echo "quality_gate_status=passed" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Quality Gate FAILED"
            echo "quality_gate_status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi

  # PHASE 14: Deployment to Staging/Production
  deploy:
    name: 'Deploy to ${{ needs.setup.outputs.target_environment }}'
    runs-on: ubuntu-latest
    needs: [setup, quality_gate, container_build]
    if: always() && needs.setup.outputs.should_deploy == 'true' && (needs.quality_gate.result == 'success' || env.QUALITY_GATE_ENABLED != 'true')
    environment: ${{ needs.setup.outputs.target_environment }}
    
    steps:
      - name: 'Checkout Code'
        uses: actions/checkout@v4

      - name: 'Setup Deployment Tools'
        run: |
          echo "üõ†Ô∏è Setting up deployment tools..."
          # Install kubectl, helm, etc.
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          
          curl https://get.helm.sh/helm-v3.13.0-linux-amd64.tar.gz | tar xz
          sudo mv linux-amd64/helm /usr/local/bin/
          echo "‚úÖ Deployment tools ready"

      - name: 'Configure Kubernetes Access'
        run: |
          echo "‚öôÔ∏è Configuring Kubernetes access..."
          mkdir -p ~/.kube
          echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > ~/.kube/config
          kubectl config current-context
          echo "‚úÖ Kubernetes access configured"

      - name: 'Pre-deployment Health Check'
        run: |
          echo "üè• Running pre-deployment health check..."
          kubectl get nodes
          kubectl get pods -n ${{ needs.setup.outputs.target_environment }}
          echo "‚úÖ Cluster is healthy"

      - name: 'Rolling Deployment'
        if: needs.setup.outputs.deployment_strategy == 'rolling'
        run: |
          echo "üîÑ Executing rolling deployment..."
          helm upgrade --install pavemaster-suite ./helm/pavemaster-suite \
            --namespace ${{ needs.setup.outputs.target_environment }} \
            --set image.tag=${{ needs.setup.outputs.version }} \
            --set image.repository=${{ env.DOCKER_REGISTRY }}/${{ github.repository }}/${{ env.IMAGE_NAME }} \
            --set replicaCount=3 \
            --set strategy.type=RollingUpdate \
            --set strategy.rollingUpdate.maxSurge=1 \
            --set strategy.rollingUpdate.maxUnavailable=0 \
            --timeout=${{ env.DEPLOYMENT_TIMEOUT }}s \
            --wait
          echo "‚úÖ Rolling deployment completed"

      - name: 'Blue-Green Deployment'
        if: needs.setup.outputs.deployment_strategy == 'blue_green'
        run: |
          echo "üîµüü¢ Executing blue-green deployment..."
          
          # Deploy to green environment
          helm upgrade --install pavemaster-suite-green ./helm/pavemaster-suite \
            --namespace ${{ needs.setup.outputs.target_environment }} \
            --set image.tag=${{ needs.setup.outputs.version }} \
            --set image.repository=${{ env.DOCKER_REGISTRY }}/${{ github.repository }}/${{ env.IMAGE_NAME }} \
            --set service.name=pavemaster-suite-green \
            --timeout=${{ env.DEPLOYMENT_TIMEOUT }}s \
            --wait
          
          # Health check green environment
          sleep 30
          kubectl wait --for=condition=ready pod -l app=pavemaster-suite-green -n ${{ needs.setup.outputs.target_environment }} --timeout=300s
          
          # Switch traffic to green
          kubectl patch service pavemaster-suite -n ${{ needs.setup.outputs.target_environment }} -p '{"spec":{"selector":{"version":"green"}}}'
          
          # Cleanup blue environment
          sleep 60
          helm uninstall pavemaster-suite-blue -n ${{ needs.setup.outputs.target_environment }} || true
          
          echo "‚úÖ Blue-green deployment completed"

      - name: 'Canary Deployment'
        if: needs.setup.outputs.deployment_strategy == 'canary'
        run: |
          echo "üê§ Executing canary deployment..."
          
          # Deploy canary version (10% traffic)
          helm upgrade --install pavemaster-suite-canary ./helm/pavemaster-suite \
            --namespace ${{ needs.setup.outputs.target_environment }} \
            --set image.tag=${{ needs.setup.outputs.version }} \
            --set image.repository=${{ env.DOCKER_REGISTRY }}/${{ github.repository }}/${{ env.IMAGE_NAME }} \
            --set replicaCount=1 \
            --set canary.enabled=true \
            --set canary.weight=10 \
            --timeout=${{ env.DEPLOYMENT_TIMEOUT }}s \
            --wait
          
          # Monitor canary for 5 minutes
          echo "üìä Monitoring canary deployment..."
          sleep 300
          
          # Check canary metrics (error rate, response time)
          CANARY_ERROR_RATE=$(kubectl exec -n monitoring deployment/prometheus -- promtool query instant 'rate(http_requests_total{job="pavemaster-suite-canary",code!~"2.."}[5m]) / rate(http_requests_total{job="pavemaster-suite-canary"}[5m])' | grep -o '[0-9.]*' | head -1)
          
          if (( $(echo "$CANARY_ERROR_RATE < 0.01" | bc -l) )); then
            echo "‚úÖ Canary metrics healthy, promoting to full deployment"
            helm upgrade pavemaster-suite-canary ./helm/pavemaster-suite \
              --set canary.weight=100 \
              --reuse-values
          else
            echo "‚ùå Canary metrics unhealthy, rolling back"
            helm rollback pavemaster-suite-canary -n ${{ needs.setup.outputs.target_environment }}
            exit 1
          fi
          
          echo "‚úÖ Canary deployment completed"

      - name: 'Post-deployment Health Check'
        run: |
          echo "üè• Running post-deployment health check..."
          
          # Wait for deployment to be ready
          kubectl wait --for=condition=available deployment/pavemaster-suite -n ${{ needs.setup.outputs.target_environment }} --timeout=${{ env.HEALTH_CHECK_TIMEOUT }}s
          
          # Check application health endpoint
          HEALTH_URL="https://pavemaster-${{ needs.setup.outputs.target_environment }}.example.com/health"
          for i in {1..10}; do
            if curl -f $HEALTH_URL; then
              echo "‚úÖ Application health check passed"
              break
            else
              echo "‚è≥ Health check attempt $i/10 failed, retrying..."
              sleep 30
            fi
            
            if [ $i -eq 10 ]; then
              echo "‚ùå Health check failed after 10 attempts"
              exit 1
            fi
          done

      - name: 'Update Deployment Status'
        run: |
          echo "üìù Updating deployment status..."
          kubectl annotate deployment pavemaster-suite -n ${{ needs.setup.outputs.target_environment }} \
            deployment.kubernetes.io/revision-history-limit=10 \
            deployment.pavemaster.io/version=${{ needs.setup.outputs.version }} \
            deployment.pavemaster.io/deployed-by="${{ github.actor }}" \
            deployment.pavemaster.io/deployed-at="$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            deployment.pavemaster.io/commit-sha="${{ github.sha }}" \
            deployment.pavemaster.io/strategy="${{ needs.setup.outputs.deployment_strategy }}"
          echo "‚úÖ Deployment status updated"

  # PHASE 14: Post-deployment Testing
  post_deployment_tests:
    name: 'Post-deployment Testing'
    runs-on: ubuntu-latest
    needs: [setup, deploy]
    if: always() && needs.deploy.result == 'success'
    
    steps:
      - name: 'Checkout Code'
        uses: actions/checkout@v4

      - name: 'Setup Node.js'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: 'Install Dependencies'
        run: pnpm install --frozen-lockfile

      - name: 'Smoke Tests'
        run: |
          echo "üí® Running smoke tests..."
          ENVIRONMENT_URL="https://pavemaster-${{ needs.setup.outputs.target_environment }}.example.com"
          pnpm run test:smoke --baseURL=$ENVIRONMENT_URL
          echo "‚úÖ Smoke tests passed"

      - name: 'API Integration Tests'
        run: |
          echo "üîå Running API integration tests..."
          ENVIRONMENT_URL="https://pavemaster-${{ needs.setup.outputs.target_environment }}.example.com"
          pnpm run test:api --baseURL=$ENVIRONMENT_URL
          echo "‚úÖ API integration tests passed"

      - name: 'Performance Regression Tests'
        run: |
          echo "üìà Running performance regression tests..."
          ENVIRONMENT_URL="https://pavemaster-${{ needs.setup.outputs.target_environment }}.example.com"
          lighthouse $ENVIRONMENT_URL --output=json --output-path=post-deploy-lighthouse.json
          
          # Compare with baseline
          PERFORMANCE_SCORE=$(cat post-deploy-lighthouse.json | jq '.lhr.categories.performance.score')
          if (( $(echo "$PERFORMANCE_SCORE >= 0.8" | bc -l) )); then
            echo "‚úÖ Performance regression test passed: $PERFORMANCE_SCORE"
          else
            echo "‚ùå Performance regression detected: $PERFORMANCE_SCORE"
            exit 1
          fi

      - name: 'Security Regression Tests'
        run: |
          echo "üîí Running security regression tests..."
          ENVIRONMENT_URL="https://pavemaster-${{ needs.setup.outputs.target_environment }}.example.com"
          
          # OWASP ZAP baseline scan
          docker run -t owasp/zap2docker-stable zap-baseline.py -t $ENVIRONMENT_URL || true
          echo "‚úÖ Security regression tests completed"

  # PHASE 14: Notification and Reporting
  notify:
    name: 'Notification & Reporting'
    runs-on: ubuntu-latest
    needs: [setup, quality_gate, deploy, post_deployment_tests]
    if: always()
    
    steps:
      - name: 'Determine Overall Status'
        id: status
        run: |
          if [[ "${{ needs.deploy.result }}" == "success" && "${{ needs.post_deployment_tests.result }}" == "success" ]]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=üéâ Deployment successful!" >> $GITHUB_OUTPUT
          elif [[ "${{ needs.quality_gate.result }}" == "failure" ]]; then
            echo "status=quality_gate_failed" >> $GITHUB_OUTPUT
            echo "message=‚ùå Quality gate failed" >> $GITHUB_OUTPUT
          elif [[ "${{ needs.deploy.result }}" == "failure" ]]; then
            echo "status=deployment_failed" >> $GITHUB_OUTPUT
            echo "message=‚ùå Deployment failed" >> $GITHUB_OUTPUT
          else
            echo "status=tests_failed" >> $GITHUB_OUTPUT
            echo "message=‚ùå Post-deployment tests failed" >> $GITHUB_OUTPUT
          fi

      - name: 'Slack Notification'
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              attachments: [{
                color: '${{ steps.status.outputs.status == "success" && "good" || "danger" }}',
                title: 'PaveMaster Suite Deployment',
                fields: [{
                  title: 'Environment',
                  value: '${{ needs.setup.outputs.target_environment }}',
                  short: true
                }, {
                  title: 'Version',
                  value: '${{ needs.setup.outputs.version }}',
                  short: true
                }, {
                  title: 'Strategy',
                  value: '${{ needs.setup.outputs.deployment_strategy }}',
                  short: true
                }, {
                  title: 'Status',
                  value: '${{ steps.status.outputs.message }}',
                  short: true
                }, {
                  title: 'Commit',
                  value: '<https://github.com/${{ github.repository }}/commit/${{ github.sha }}|${{ needs.setup.outputs.short_sha }}>',
                  short: true
                }, {
                  title: 'Actor',
                  value: '${{ github.actor }}',
                  short: true
                }]
              }]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        if: always()

      - name: 'Email Notification'
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 465
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: 'PaveMaster Suite Deployment - ${{ steps.status.outputs.status }}'
          body: |
            Deployment Summary:
            
            Environment: ${{ needs.setup.outputs.target_environment }}
            Version: ${{ needs.setup.outputs.version }}
            Strategy: ${{ needs.setup.outputs.deployment_strategy }}
            Status: ${{ steps.status.outputs.message }}
            
            Commit: ${{ github.sha }}
            Actor: ${{ github.actor }}
            
            View details: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          to: devops@company.com
        if: always() && (steps.status.outputs.status != 'success' || needs.setup.outputs.target_environment == 'production')

      - name: 'Create Deployment Record'
        run: |
          echo "üìù Creating deployment record..."
          curl -X POST \
            -H "Authorization: Bearer ${{ secrets.DEPLOYMENT_API_TOKEN }}" \
            -H "Content-Type: application/json" \
            -d '{
              "environment": "${{ needs.setup.outputs.target_environment }}",
              "version": "${{ needs.setup.outputs.version }}",
              "strategy": "${{ needs.setup.outputs.deployment_strategy }}",
              "status": "${{ steps.status.outputs.status }}",
              "commit_sha": "${{ github.sha }}",
              "deployed_by": "${{ github.actor }}",
              "deployed_at": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'",
              "workflow_run_id": "${{ github.run_id }}"
            }' \
            https://api.pavemaster.com/deployments
          echo "‚úÖ Deployment record created"